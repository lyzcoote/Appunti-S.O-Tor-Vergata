# 2.3 Comunicazione fra processi

**I processi necessitano frequentemente di comunicare con altri processi.** 

**C’è quindi bisogno che i processi comunichino**, preferibilmente in un modo ben strutturato, non usando gli interrupt. 

**Ma esistono dei problemi relativi a questa comunicazione fra processi o `IPC`** (InterProcess Communication).

Molto brevemente, **i problemi da affrontare sono tre**:

-   Come un processo possa **passare informazioni a un altro**. 

-   L’essere sicuri che **due o più processi non vadano ad accavallarsi**, cioè che entrambi i processi lottano per leggere/modificare un specifico valore in memoria. 

-   Il terzo riguarda la **corretta sequenzialità** quando vi sono delle dipendenze.

    -   >**E.g:** Se il processo A produce dei dati e il processo B li stampa, B deve attendere che A abbia prodotto qualche dato prima di iniziare a stampare.

&nbsp;
&nbsp;
&nbsp;


>**NOTA:** È anche importante menzionare che due di questi problemi si possono applicare ugualmente bene anche ai thread. 

Vediamo un po':

-   **Il primo** (passare le informazioni):

    -   **Non è un problema per i thread**, dato che condividono uno spazio degli indirizzi comune.
     
-   **Il secondo** (non intromettersi l’uno nell’altro) e **il Terzo** (la corretta sequenza)

    -   **Questi problemi valgono anche per i thread.**


&nbsp;
&nbsp;
&nbsp;

*Race Condition*
====

**La situazione di corsa (`race condition`), è un fenomeno che si presenta nei sistemi concorrenti quando**, in un sistema basato su processi multipli, **il risultato finale dell'esecuzione di una serie di processi dipende dalla temporizzazione o dalla sequenza con cui vengono eseguiti.**

Spesso, una situazione di corsa **è un effetto indesiderato e genera malfunzionamenti.**

 In questo caso, essa è denominata corsa critica per il sistema.

**Per evitare il verificarsi di situazioni di corsa** quando si impiegano memorie, file o risorse in condivisione, **sono stati studiati diversi algoritmi che prevedono la mutua esclusione**, ovvero **garantiscono che, quando la risorsa condivisa è interessata un processo, nessun altro processo possa accedervi.**

Se più processi hanno la possibilità di accedere ad una risorsa in modalità di scrittura, **è importante prevedere ed includere l'utilizzo di questi algoritmi**, dei quali **si può invece fare a meno se la risorsa è accessibile solamente in modalità di lettura**, perché i processi non possono influenzare lo stato della risorsa, quindi non è fisicamente possibile la realizzazione di situazioni di corsa.

&nbsp;
&nbsp;
&nbsp;

*Regioni critiche*
====

Come evitare le race condition? La chiave per prevenire i problemi in questo caso e in molte altre situazioni riguardanti:

-   La memoria condivisa

-   File condivisi 

-   Altri elementi condivisi 

è trovare un qualche modo per proibire a più di un processo di leggere e scrivere dati condivisi contemporaneamente. 

Ovvero un qualche sistema per essere certi che, se un processo sta usando una variabile o un file condivisi, l’altro processo venga escluso dal fare la stessa cosa. 

**La difficoltà sta nel fatto che, per esempio, il processo B ha iniziato a usare una delle variabili condivise prima che il processo A avesse terminato di usarla.**

Però, **possiamo notare che un processo è, per parte del tempo, impegnato in calcoli interni e altre attività che non portano a race condition.** 

Tuttavia, **un processo deve talvolta accedere alla memoria o a file condivisi** o svolgere attività critiche **che possono portare alle race condition.**

**La parte di programma in cui si accede alla memoria condivisa è chiamata regione critica o sezione critica.**

Se potessimo fare in modo **che due processi non si trovino mai nelle loro regioni critiche allo stesso momento, avremmo trovato la soluzione alle race condition.**

Per ottenere una buona soluzione servono **quattro condizioni**:

-   **Due processi non devono mai trovarsi contemporaneamente nelle loro regioni critiche;**

-   **Non può essere fatto alcun presupposto sulle velocità o sul numero delle CPU;**

-   **Nessun processo in esecuzione al di fuori della sua regione critica può bloccare altri processi;**

-   **Nessun processo dovrebbe restare in attesa di entrare nella sua regione critica per ­sempre.**


### **`Mutua esclusione usando le regioni critiche.`**

![alt text](https://i.imgur.com/Uk6KL7K.png)

&nbsp;
&nbsp;
&nbsp;

*Mutua esclusione con busy waiting*
====

Adesso esamineremo varie proposte per ottenere la mutua esclusione in modo che, mentre un processo è occupato nell’aggiornare la propria memoria condivisa nella sua regione critica, nessun altro processo entri nella propria regione critica e causi problemi.

**Busy Waiting**

-   Soluzione generale: si fa aspettare chi deve entrare quando la sezione critica è già occupata, valutando continuamente una variabile: `Busy waiting`

-   Si usano variabili di controllo condivise (`lock`) lette prima di entrare e scritte appena entrati in sezione critica.

>**E.g:** Se si esegue un installazione di un pacchetto tramite APT su una Debian based su un processo A e un altro processo B tenta di installare un altro pacchetto sempre tramite APT, il processo B aspetterà che la variabile di `lock` si liberi dal processo A


### **`Esempio di Busy Waiting.`**
![alt text](https://i.imgur.com/pQv7ae9.png)

****

&nbsp;
&nbsp;
&nbsp;

**Disabilitare gli interrupt**

Si può garantire l’accesso in mutua esclusione ad una sezione critica disabilitando gli interrupt.

**Pro:**

-   Utile per il kernel per aggiornare le sue variabili

**Contro:**

-   Non è raccomandabile dare all’utente il potere di disabilitare gli interrupt:

    -   Dato che è un istruzione privilegiata.

    -   L’utente potrebbe non riabilitarli più e non cedere più la CPU

-   Rallenta la risposta agli eventi

-   Inutile in un sistema multiprocessore

-   Non è strutturata e rende di difficile comprensione il codice

****

&nbsp;
&nbsp;
&nbsp;

**Soluzione di Peterson**


L'algoritmo di Peterson o algoritmo `tie-breaker` **è un algoritmo sviluppato nella teoria del controllo della concorrenza per coordinare due o più processi o thread che hanno una sezione critica in cui deve esservi mutua esclusione.**

**L'algoritmo assicura la corretta sincronizzazione, impedendo lo stallo (`deadlock`) ed assicurando che soltanto un processo alla volta possa eseguire una sezione critica (serializzazione).**

>**E.g:** Algoritmo con due processi usando `pseudocodice C`

```
 //'' dichiarazione delle variabili globali comuni''
 boolean in1 = false, in2 = false;
 int turno;
 
 // processo #1
 Process CS1 {
     while(1) {
         in1 = true;
         turno = 2;
         while (in2 && turno == 2)
            ; // finché è il turno del processo #2, il processo #1 rimane all'interno del while
         <sezione critica 1>
         in1 = false;
         <sezione non critica 1>
     }
 }
 
 // processo #2
 Process CS2 {
     while(1) {
         in2 = true;
         turno = 1;
         while (in1 && turno == 1)
            ; // finché è il turno del processo #1, il processo #2 rimane all'interno del while
         <sezione critica 2>
         in2 = false;
         <sezione non critica 2>
      }
 }
```

>**Funzionamento:**

**Se il processo #1 viene eseguito per primo, `in1` viene impostato a `true` *prima* di impostare `turno` a ``2.** 
Dal momento che `in2` è stato inizializzato a `false`, la condizione dell'iterazione while non è soddisfatta e il processo #1 accede alla sezione critica.

**Se nel frattempo viene avviato il processo #2, questo imposterà `in2` a `true` e `turno` a `1`.** **`in1` è già stato impostato a `true` dal processo #1**, perciò **la condizione dell'iterazione `while del processo #2` è soddisfatta**, così che questo deve aspettare. **Solo dopo che il processo #1 ha abbandonato la sezione critica `in1` diventa `false`, e il processo #2 può accedere alla sua sezione critica.**

**Se il processo #1 viene nel frattempo riavviato, imposterà `turno` a `2`**, e **dovrà aspettare che il processo #2 abbia abbandonato la sua sezione critica.**

-   La m**utua esclusione è garantita** dal fatto che **il controllo di `in` e di `turno` viene fatto in modo atomico.**

-  **L'assenza di deadlock viene garantita** dal fatto che solamente **una delle due condizioni while può essere vera nello stesso momento.**

-  **Il progresso dell'elaborazione è garantito** dal fatto che se uno dei due processi tenta di entrare e l'altro non è in sezione critica può tranquillamente entrare.

-   **L'attesa di un processo è limitata** in quanto, se i due processi tentano di entrare in sezione critica, entrano alternamente.

****

&nbsp;
&nbsp;
&nbsp;

**Semaforo**

Il semaforo è uno strumento di sincronizzazione dei processi.

Il semaforo è in genere una variabile intera S che viene inizializzata al numero di risorse presenti nel sistema e il valore del semaforo può essere modificato solo da due funzioni wait() e signal() oltre all’inizializzazione.

L’operazione wait() e signal() modifica indivisibilmente il valore del semaforo. Significa che quando un processo modifica il valore del semaforo, nessun altro processo può modificare contemporaneamente il valore del semaforo. 

I semafori si distinguono per il sistema operativo in due categorie: il semaforo intero e il semaforo binario.

Semaforo intero:

-   Nel semaforo intero, il valore del semaforo S viene inizializzato al numero di risorse presenti nel sistema. 

-   Ogni volta che un processo vuole accedere alla risorsa, esegue un’operazione wait() sul semaforo e diminuisce di uno il valore del semaforo. 

-   Quando rilascia la risorsa, esegue l’operazione signal() sul semaforo e incrementa il valore del semaforo di uno.

-   Quando il semaforo intero va a 0, significa che tutte le risorse sono occupate dai processi. Se un processo deve utilizzare una risorsa quando il semaforo intero è 0, esegue wait() e viene bloccato fino a quando il valore del semaforo diventa maggiore di 0.

Semaforo binario

-   Nel semaforo binario, il valore del semaforo è compreso tra 0 e 1. 

-   È simile al blocco mutex, ma il mutex è un meccanismo di blocco, mentre il semaforo è un meccanismo di segnalazione.

-   Nel semaforo binario, se un processo vuole accedere alla risorsa esegue un’operazione wait() sul semaforo e decrementa il valore del semaforo da 1 a 0. 

-   Quando rilascia la risorsa, esegue un’operazione signal() sul semaforo e incrementi il suo valore su 1. 

-   Se il valore del semaforo è 0 e un processo desidera accedere alla risorsa, esegue un’operazione wait() e si blocca fino a quando il processo corrente che utilizza le risorse rilascia la risorsa

****

&nbsp;
&nbsp;
&nbsp;

**Mutex**

**L’oggetto di esclusione reciproca è chiamato `Mutex`.** 

**Precedentemente solo un processo alla volta poteva accedere a una determinata risorsa.**

L’oggetto mutex **consente ai thread di più programmi di utilizzare la stessa risorsa ma uno alla volta non contemporaneamente.**

All’avvio di un programma, **richiede al sistema di creare un oggetto mutex per una determinata risorsa**. 

**Il sistema crea l’oggetto mutex con un nome o ID univoco.** 

**Ogni volta che il thread del programma desidera utilizzare la risorsa che occupa il blocco sull’oggetto mutex, utilizza la risorsa e dopo l’uso rilascia il blocco sull’oggetto mutex.** 

Quindi al **processo successivo è consentito acquisire il blocco sull’oggetto mutex.**

Nel frattempo, un processo ha acquisito il blocco sull’oggetto mutex a cui nessun altro thread/processo può accedere a quella risorsa. 

Se l’oggetto mutex è già bloccato, **il processo che desidera acquisire il blocco** sull’oggetto mutex **deve attendere e viene messo in coda dal sistema** fino a quando l’oggetto mutex non viene sbloccato.

****

&nbsp;
&nbsp;
&nbsp;

**Futex**

Un futex è una caratteristica che implementa un sistema di lock molto basico (**simile a un mutex**) tipica di Linux, **senza però andare a toccare il kernel, a meno che non sia assolutamente necessario.**

**Poiché passare al kernel e quindi tornare è un’operazione onerosa, questo comportamento migliora sensibilmente le prestazioni.**

Un futex è costituito di due parti:

-   Un servizio kernel:

    -   Il servizio kernel fornisce una `coda d’attesa` che consente a più processi di attendere un `lock`.

    -   I processi non sono in esecuzione a meno che il kernel non li sblocchi esplicitamente. 

    -   Inserire un processo nella coda d’attesa richiede una chiamata di sistema piuttosto onerosa; è quindi meglio evitarlo. 

    -   In assenza di contese, pertanto, il futex opera interamente in spazio utente; in particolare, i processi condividono una variabile lock comune (un intero allineato a 32 bit che funge da lock).

-   Una libreria utente.


### **`Esempio di Futex.`**
****

Supponiamo che, inizialmente, **il lock abbia valore 1, ossia che sia libero.** 

**Un thread si appropria del lock** eseguendo un’istruzione `decrement and test`.

**Il thread verifica quindi i risultati per controllare se il lock era libero o no:**

-   **Se non era in stato locked:**

    -   Va tutto bene e il thread è riuscito ad appropriarsi del lock. 

-   **Se invece il lock è detenuto da un altro thread:**

    -   Il nostro thread deve attendere. 
    
    -   In questo caso, la libreria del futex non entra in spin lock, ma utilizza una chiamata di sistema per posizionare il thread nella coda di attesa all’interno del kernel.

A questo punto, **il costo del passaggio al kernel dovrebbe essere giustificato**, perché il thread sarebbe stato comunque bloccato. 

**Quando un thread ha terminato di utilizzare il lock, lo rilascia con un’istruzione atomica `increment and test`** e **verifica il risultato per sapere se nella coda di attesa del kernel sono presenti altri processi bloccati.**

-   **In caso positivo (ci sono contese):**

    -  *Chiama il Kernel in causa e gli fa sapere che può sbloccare uno o più di questi processi.

-   **In caso negativo (non ci siano contese):**

-   Il kernel non viene chiamato in causa.
****



